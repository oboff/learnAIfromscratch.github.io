<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2021-01-07 Thu 11:07 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>An Artificial Intelligence Curriculum</title>
<meta name="generator" content="Org mode">
<meta name="author" content="jbh">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
   body {
    background-color: #ededed;
    margin: auto;
    padding: 0.5rem;
    max-width: 80ch; 
  }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<header>
<h1 class="title">An Artificial Intelligence Curriculum</h1>
</header><nav id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org80c78d1">Intro</a>
<ul>
<li><a href="#orgf64bcc0">Materials</a></li>
</ul>
</li>
<li><a href="#org6053fa6">Start here</a>
<ul>
<li><a href="#orgc8dd548">Probability</a></li>
</ul>
</li>
</ul>
</div>
</nav>

<div id="outline-container-org80c78d1" class="outline-2">
<h2 id="org80c78d1">Intro</h2>
<div class="outline-text-2" id="text-org80c78d1">
<p>
This is a self-directed workshop to learn machine learning, neural networks, autonomous agents and whatever else we can find. Like the other two workshops on <a href="https://learnaifromscratch.github.io/math.html">math</a> and <a href="https://learnaifromscratch.github.io/software.html">computer science</a> this is a course built around courses. Self-directed means choose your own curriculum. I'll audit resources and you can decide to take it yourself or not. My interest will be: learning some of the popular current models and methods for learning, how to <a href="https://mitmath.github.io/18337/">optimize</a> these models and deploy them, and <a href="https://ai6034.mit.edu/wiki/index.php?title=6.S966:_A_Graduate_Section_for_6.034#Prospectus">criticism</a> of the current state of AI to see if there's something else we should be trying. This field changes every year as you will see in the first 10-701 lecture, methods that we abandoned have a breakthrough then we go back to them, and methods that dominate today could be replaced tomorrow.     
</p>
</div>

<div id="outline-container-orgf64bcc0" class="outline-3">
<h3 id="orgf64bcc0">Materials</h3>
<div class="outline-text-3" id="text-orgf64bcc0">
<p>
As I go I will choose various materials from the following list that will probably grow over time:
</p>

<ul class="org-ul">
<li><a href="https://www.depthfirstlearning.com/">Depth First Learning</a> are curriculums built around a single paper. You can learn exactly how AlphaGo and other game solving techniques work, neural networks, different training methods. Many of these paper curriculums were funded by Jane Street Capital so have applications in quantitative finance.</li>
<li>10-701 (Fall 2020) <a href="https://www.cs.cmu.edu/~epxing/Class/10701-20/schedule.html">Intro to Machine Learning</a> has YouTube recorded lectures and is CMU's graduate course in machine learning. The difference between CMU ML intro courses is <a href="https://docs.google.com/document/d/1Y0Jx_tcINWQrWJx31WGEQSsUs059OUMmPIVSeyxNdeM/edit">here</a>. It's also a survey of methods by current researchers so they will talk about the latest models and methods, what doesn't work for what.</li>
<li>10-301/601 (Spring 2020) <a href="http://www.cs.cmu.edu/~mgormley/courses/10601/schedule.html">Intro to Machine Learning</a> also has YouTube <a href="https://www.youtube.com/playlist?list=PLpqQKYIU-snAPM89YPPwyQ9xdaiAdoouk">recorded lectures</a> with highly polished slides and lectures.</li>
<li>15-388 (2019) <a href="https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22618ea253-ca45-4b14-9f1d-aab501543bd2%22">Practical Data Science</a> another CMU course has good lectures to use as recitations for prereqs like linear algebra or probability, and the <a href="http://www.datasciencecourse.org/lectures/">notes</a> are in interactive jupyter notebooks you can view or run yourself as you read them.</li>
<li>CS4780 (2018) <a href="https://www.youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS">Machine Learning for Intelligent Systems</a> Cornell's intro ML class has the best instructor I find in terms of explaining the math notation, keeping on point (no long offtopic anecdotes), and has the best <a href="http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/">notes</a>. This class only covers supervised learning which has overlap with unsupervised and reinforcement learning.</li>
<li>Many book chapters, papers, guides all suggested by these courses&#x2026; primarily the Deep Learning book by Goodfellow, Mathematics for Machine Learning, and Strang's new Linear Algebra w/Deep Learning.</li>
<li>A <a href="https://computationalthinking.mit.edu/Fall20/">course</a> on programming models so we can do things like import any Python or R library we want and use it, and learn how to optimize our programs when deploying them instead of every other ML curriculum that stops with the modelling and tells you nothing about GPUs or parallel deployment.</li>
</ul>

<p>
Maybe try some AI competitions, like <a href="https://halite.io/">Halite</a>. 
</p>
</div>
</div>
</div>

<div id="outline-container-org6053fa6" class="outline-2">
<h2 id="org6053fa6">Start here</h2>
<div class="outline-text-2" id="text-org6053fa6">
<p>
Let's audit the first intro lecture for CS4780, 10-701, and 10-601. The learning goals of 10-601 are <a href="http://www.cs.cmu.edu/~mgormley/courses/10601/slides/10601-objectives.pdf">here</a>, it's first lecture primarily talked about history of ML like expert systems and why they failed. We learn the brief difference between regression output, probability output and classification. We are told there are sets T, P and E where T is the set of tasks, P is the set of performance measures that improves with experiences in set E. Most of this lecture and 10-701 are course logistics, you can watch both in under 30mins skipping the logistics. 10-701 talks about how ML methods change often, there was a breakthrough in neural networks recently so now it's all about neural networks, we used to heavily use kernel methods and now not as much. We could use convex optimization back then and now things are much more non-convex.  
</p>

<p>
Meanwhile in CS4780 the <a href="https://youtu.be/MrLPzBxG95I">intro</a> lecture jumps into supervised learning setup, explaining the math notation. His high level explanation of how machine learning works: insert data, insert expected output, generate program. At 32:00 the supervised learning intro begins. Watch this first to see what we need to know then we will pick up all the probability and linear algebra after. Lecture notes are <a href="http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/">here</a>. He uses arrows on top of the notation to denote vectors. Look up what <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">iid</a> is: "all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples". \(D = \{ (x_{i}, y_{i})....(x_{n},y_{n}) \}\) is a subset of XY meaning a cartesian product of every single combination of X (features) with Y (labels), like the cartesian coordinates in a x and y graph, you can combine any one of them to form a point: (0,1) or (100,-1). So using his patient data vector example, Y would be binary 0 or 1 to represent yes or no. X is all the feature set of a patient and D is the total training data set of all possible combinations of feature set with yes and no. In the notes this notation is \(R^d \cdot C\) and we can think of this notation as the type of the elements inside D, everything we insert into D is a tuple of (features, labels).
</p>

<p>
You can learn basic set theory by typing it into <a href="https://www.youtube.com/watch?v=59WX2V7Vjgg">YouTube</a> or going through the math workshop, which also covers what a vector is, and reading Terence Tao's <i>Analysis I</i> chapter on sets. I will now assume you know/can look up any set theory we see in these lectures.   
</p>
</div>

<div id="outline-container-orgc8dd548" class="outline-3">
<h3 id="orgc8dd548">Probability</h3>
<div class="outline-text-3" id="text-orgc8dd548">
<p>
My skills in probability are weak so I'm going to research around to find some material on it. A good starting point is 15-251 CMU's undergrad class on theoretical cs topics. Let's watch <a href="https://youtu.be/gtNNEPuyVXo">Probability 1</a> and <a href="https://youtu.be/p7fqy9FLA8A">Probability 2</a> lectures. I'm also going through his <a href="https://www.cs.cmu.edu/~odonnell/papers/probability-and-computing-lecture-notes.pdf">notes</a> on probability because they're pretty good at explaining this from a computer science viewpoint "probability theory = analyzing randomized code". By this I mean I watched both lectures, then will go through these notes as I do machine learning not completely master the entire set of 252 pages of notes and then resume, you can do both at the same time. 
</p>

<p>
I'm watching the 2013 YouTube lecture but if you want the 2015 Panopto version it is <a href="https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6a3880a8-fbf8-4c10-8ebb-a7ca8360a0ba&amp;start=undefined">here</a>. In the YouTube lecture we are interrupted by an ancient <a href="https://youtu.be/DFB4Iw7BHoU">meme</a>. The input to these algorithms he uses to set up the theory are 1/m with m being the total amount of possible outcomes. A coin flip has two outcomes: it's probability of heads or tails is 1 over 2 possible outcomes or 1/2. The probability tree, if you haven't figured it out you start with 1/2 up top and now each outcome must add up to 1, so RanInt(3) left side is 3/6 or 1/2, which when combined with Bernoulli(1/2) is 1. TODO
</p>
</div>
</div>
</div>
</div>
</body>
</html>
