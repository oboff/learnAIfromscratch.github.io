<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2021-01-21 Thu 10:04 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>An Artificial Intelligence Curriculum</title>
<meta name="generator" content="Org mode">
<meta name="author" content="jbh">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  body {
  max-width: 40rem;
  padding: 1rem;
  margin: auto;
  }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<style> body {background-color: #fafad2} </style>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<header>
<h1 class="title">An Artificial Intelligence Curriculum</h1>
</header><nav id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#Intro">Intro</a>
<ul>
<li><a href="#Materials">Materials</a></li>
</ul>
</li>
<li><a href="#Start%20here">Start here</a>
<ul>
<li><a href="#Probability">Probability</a>
<ul>
<li><a href="#Lecture%201%2015-251">Lecture 1 15-251</a></li>
<li><a href="#Lecture%202%2015-251">Lecture 2 15-251</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</nav>

<div id="outline-container-Intro" class="outline-2">
<h2 id="Intro">Intro</h2>
<div class="outline-text-2" id="text-Intro">
<p>
This is a self-directed workshop to learn machine learning, neural networks, autonomous agents and whatever else we can find. Like the other two workshops on <a href="https://learnaifromscratch.github.io/math.html">math</a> and <a href="https://learnaifromscratch.github.io/software.html">computer science</a> this is a course built around courses. Self-directed means choose your own curriculum. I'll audit resources and you can decide to take it yourself or not. My interest will be: learning some of the popular current models and methods for learning, how to <a href="https://mitmath.github.io/18337/">optimize</a> these models and deploy them, and <a href="https://ai6034.mit.edu/wiki/index.php?title=6.S966:_A_Graduate_Section_for_6.034#Prospectus">criticism</a> of the current state of AI to see if there's something else we should be trying.
</p>

<p>
I will go through some of the lectures and papers, and some of the prereqs like picking up probability, matrix manipulations, calculus. You can then figure the rest out yourself easily really the only issue is getting to a level where all these abstract models and notation make sense then you can coast to the finish line. 
</p>
</div>

<div id="outline-container-Materials" class="outline-3">
<h3 id="Materials">Materials</h3>
<div class="outline-text-3" id="text-Materials">
<p>
As I go I will choose various materials from the following list that will probably grow over time:
</p>

<ul class="org-ul">
<li><a href="https://www.depthfirstlearning.com/">Depth First Learning</a> are curriculums built around a single paper. You can learn exactly how AlphaGo and other game solving techniques work, neural networks, different training methods. Many of these paper curriculums were funded by Jane Street Capital so have applications in quantitative finance.</li>
<li>10-701 (Fall 2020) <a href="https://www.cs.cmu.edu/~epxing/Class/10701-20/schedule.html">Intro to Machine Learning</a> has YouTube recorded lectures and is CMU's graduate course in machine learning. The difference between CMU ML intro courses is <a href="https://docs.google.com/document/d/1Y0Jx_tcINWQrWJx31WGEQSsUs059OUMmPIVSeyxNdeM/edit">here</a>. It's also a survey of methods by current researchers so they will talk about the latest models and methods, what doesn't work for what.</li>
<li>10-301/601 (Spring 2020) <a href="http://www.cs.cmu.edu/~mgormley/courses/10601/schedule.html">Intro to Machine Learning</a> also has YouTube <a href="https://www.youtube.com/playlist?list=PLpqQKYIU-snAPM89YPPwyQ9xdaiAdoouk">recorded lectures</a> with highly polished slides and lectures.</li>
<li>15-388 (2019) <a href="https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22618ea253-ca45-4b14-9f1d-aab501543bd2%22">Practical Data Science</a> another CMU course has good lectures to use as recitations for prereqs like linear algebra or probability, and the <a href="http://www.datasciencecourse.org/lectures/">notes</a> are in interactive jupyter notebooks you can view or run yourself as you read them.</li>
<li>CS4780 (2018) <a href="https://www.youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS">Machine Learning for Intelligent Systems</a> Cornell's intro ML class has the best instructor I find in terms of explaining the math notation, keeping on point (no long offtopic anecdotes), and has the best <a href="http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/">notes</a>. This class only covers supervised learning which has overlap with unsupervised and reinforcement learning.</li>
<li>Many book chapters, papers, guides all suggested by these courses&#x2026; primarily the Deep Learning book by Goodfellow, Mathematics for Machine Learning, and Strang's new Linear Algebra w/Deep Learning.</li>
<li>A <a href="https://computationalthinking.mit.edu/Fall20/">course</a> on programming models so we can do things like import any Python or R library we want and use it, and learn how to optimize our programs when deploying them instead of every other ML curriculum that stops with the modelling and tells you nothing about GPUs or parallel deployment. You can of course use any language you want, even common <a href="https://github.com/numcl/numcl">lisp</a>.</li>
</ul>

<p>
Maybe try some AI competitions, like <a href="https://halite.io/">Halite</a>. 
</p>
</div>
</div>
</div>

<div id="outline-container-Start%20here" class="outline-2">
<h2 id="Start%20here">Start here</h2>
<div class="outline-text-2" id="text-Start%20here">
<p>
Let's audit the first intro lecture for CS4780, 10-701, and 10-601. The learning goals of 10-601 are <a href="http://www.cs.cmu.edu/~mgormley/courses/10601/slides/10601-objectives.pdf">here</a>, it's first lecture primarily talked about history of ML like expert systems and why they failed. We learn the brief difference between regression output, probability output and classification. We are told there are sets T, P and E where T is the set of tasks, P is the set of performance measures that improves with experiences in set E. Most of this lecture and 10-701 are course logistics, you can watch both in under 30mins skipping the logistics.
</p>

<p>
10-701 when you get past course logistics talks about how ML methods change often, there was a breakthrough in neural networks recently so now it's all about neural networks, we used to heavily use kernel methods and now not as much. We could use convex optimization back then and now things are much more non-convex.  
</p>

<p>
Meanwhile in CS4780 the <a href="https://youtu.be/MrLPzBxG95I">intro</a> lecture jumps into supervised learning setup, explaining the math notation. His high level explanation of how machine learning works: insert data, insert expected output, generate program. At 32:00 the supervised learning intro begins. Watch this first to see what we need to know then we will pick up all the probability and linear algebra after. Lecture notes are <a href="http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/">here</a>. He uses arrows on top of the notation to denote vectors. Look up what <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">iid</a> is: "all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples". \(D = \{ (x_{i}, y_{i})....(x_{n},y_{n}) \}\) is a subset of XY meaning a cartesian product of every single combination of X (features) with Y (labels), like the cartesian coordinates in a x and y graph, you can combine any one of them to form a point: (0,1) or (100,-1). So using his patient data vector example, Y would be binary 0 or 1 to represent yes or no. X is all the feature set of a patient and D is the total training data set of all possible combinations of feature set with yes and no.
</p>

<p>
You can learn basic set theory by typing it into <a href="https://www.youtube.com/watch?v=59WX2V7Vjgg">YouTube</a> or going through the math workshop, which also covers what a vector is, and reading Terence Tao's <i>Analysis I</i> chapter on sets. I will now assume you know/can look up any set theory we see in these lectures.   
</p>
</div>

<div id="outline-container-Probability" class="outline-3">
<h3 id="Probability">Probability</h3>
<div class="outline-text-3" id="text-Probability">
<blockquote>
<p>
"Probability = analyzing random code"
</p>
</blockquote>

<p>
A good starting point is 15-251 CMU's undergrad class on theoretical cs topics. Let's watch <a href="https://youtu.be/gtNNEPuyVXo">Probability 1</a> and <a href="https://youtu.be/p7fqy9FLA8A">Probability 2</a> lectures taught by Ryan O'Donnell, the full notes with exercises are <a href="https://www.anilada.com/courses/15251f17/www/notes/notes_all_but_sols.pdf">here</a> to practice after, and the solutions to the exercises to check your own work is <a href="https://www.anilada.com/courses/15251f17/www/notes/notes_exs_sols.pdf">here</a>. 
</p>

<p>
Other sources you could use 
</p>
<ul class="org-ul">
<li>These brief <a href="http://bactra.org/prob-notes/srl.pdf">notes</a></li>
<li>This whole MIT <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041sc-probabilistic-systems-analysis-and-applied-probability-fall-2013/">course</a> which has fully worked out solutions to the assignments</li>
<li>Any machine learning <a href="https://mml-book.github.io/">book</a></li>
<li>The first chapters of the deep learning <a href="https://www.deeplearningbook.org/">book</a> (use libgen to get a decent pdf, the site copy is trash on my browser)</li>
<li>Terence Tao's <a href="https://terrytao.wordpress.com/2015/09/29/275a-notes-0-foundations-of-probability-theory/">notes</a> on the foundations of probability theory, the discrete probability notes are excellent</li>
<li>Wildberger's probability and stats <a href="https://www.youtube.com/playlist?list=PLIljB45xT85AMigTyprOuf__daeklnLse">lectures</a></li>
<li>Ryan O'Donnell's <a href="https://www.cs.cmu.edu/~odonnell/papers/probability-and-computing-lecture-notes.pdf">notes</a> from 15-359 a class about random computing (figuring out what load balancing should be, random algorithms, etc).</li>
</ul>
</div>

<div id="outline-container-Lecture%201%2015-251" class="outline-4">
<h4 id="Lecture%201%2015-251">Lecture 1 15-251</h4>
<div class="outline-text-4" id="text-Lecture%201%2015-251">
<p>
I'm watching the 2013 15-251 YouTube lecture and the 2015 Panopto version <a href="https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6a3880a8-fbf8-4c10-8ebb-a7ca8360a0ba&amp;start=undefined">here</a> with unobstructed slides, and the lecture clarifies things better than the 2013 lecture, though the notes for the course cover everything. In the 2013 YouTube lecture we are interrupted by an ancient <a href="https://youtu.be/DFB4Iw7BHoU">meme</a>. The probability tree, if you haven't figured it out you start with 1/2 up top and now each outcome must add up to 1, so RandInt(3) left side is 3/6 or 1/2, which when combined with Bernoulli(1/2) is 1. The term notation overload means multiple definitions for the same notation, hoping you derive from context which definition the notation is using.
</p>

<p>
Anytime there is some mental rational arithmetic try it in your head. 1/6 + 1/8 + 1/8 is 1/6 + 1/4, which from Wildberger lectures if you watched them in math workshop, a/b + c/d is (ad + cb)/bd and or just (6+4)/24 since the numerator is all 1's. The more you practice this the easier it gets like that guy in the class who just shouts out the answer in the 2015 lecture. 
</p>

<p>
The 2015 Panopto lecture at 27:05 explains the "I will roll two dice 24 times. I win if I get a double-1s" better than the 2013 lecture. There are 36 outcomes each time he rolls 2 dice ie: outcome: 1,1 or 1,2 or 1,3 or 1,4 or 1,5 or 1,6. If you draw a tree this will end up being 36 total outcomes with each roll since 6x6. Remove the single outcome where it comes up 1,1, so 35/36 outcomes per roll and there is 24 rolls. Subtract that from the total probability of 1 in other words the \(Pr[W^{c}]\) compliment definition. The silver and gold coin problem you will get right away as 2/3 probability if you've ever seen those Monty Hall probability <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/video-lectures/lecture-18-probability-introduction/">lectures</a> from MIT's 6.042J except here I find it's explained better, later in the lecture he explains why he doesn't teach the Monty Hall example.  
</p>

<p>
2015 lecture for independence of multiple events, the huge AND notation and product notation means as you iterate through \(Pr[A_1]\) to \(Pr[A_5]\), you insert that symbol between each value. So \(Pr[A_1] \cap Pr[A_2] \cap Pr[A_3]\) .. by probability definition is the same as \(Pr[A_1] \cdot Pr[A_2] \cdot Pr[A_3]\).., but for every possible subset of {1,2,3,4,5}.
</p>

<p>
At the end of the lecture during the Birthday Paradox you can see why I chose these lectures over every other school's intro probability lectures, he avoids all the highschool-tier simplistic demonstrations of wasting time going through the class asking their birthdays and skips right to the probability of using a random day to show it works. I've seen many lectures where this same content is reduced to a magic show and then the notes are impossible to figure out whereas here he sets up this theory as random code to analyze. 
</p>
</div>
</div>

<div id="outline-container-Lecture%202%2015-251" class="outline-4">
<h4 id="Lecture%202%2015-251">Lecture 2 15-251</h4>
<div class="outline-text-4" id="text-Lecture%202%2015-251">
<p>
Random variables. The YouTube version is the same as the Panopto 2015 <a href="https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=d4ef6eb2-870e-4526-bdd8-79654572583a&amp;start=undefined">version</a>. Everything here is translated to code, W = 31 x I - 1 should be W = (31 x I) - 1, and T = 2 x (RandInt(5) - 1) assume that RandInt(0) is not possible and raises an exception, or else this code makes no sense with 2 x (0 - 1), this is in the notes.
</p>

<p>
Linearity of Expectation proofs he's just using distributive property to move stuff around, E[aX + b] is the same as (a(E[X]) + b). Note around 35:00 where he makes a comment about understanding the types. An event space can't be summed because it's a set. The indicator can be summed because it is 0 or 1, type real number. The last 10 minutes or so of this lecture he uses a graph coloring problem and talks about satisfiability, both explained in previous lectures, you can either watch them yourself or just go through the notes.  
</p>

<p>
Let's try the exercises <a href="https://www.anilada.com/courses/15251f17/www/notes/notes_all_but_sols.pdf">here</a>, then compare to the <a href="https://www.anilada.com/courses/15251f17/www/notes/notes_exs_sols.pdf">solutions</a>. TODO
</p>

<hr>
<p>
<a href="./index.html">Home</a>
</p>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
