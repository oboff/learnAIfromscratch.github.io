<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2021-05-13 Thu 22:18 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Artificial Intelligence</title>
<meta name="generator" content="Org mode">
<meta name="author" content="jbh">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  body {
  max-width: 40rem;
  padding: 1rem;
  margin: auto;
  }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<style> body {background-color: #fafad2} </style>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<header>
<h1 class="title">Artificial Intelligence</h1>
</header><nav id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#Courses">Courses</a>
<ul>
<li><a href="#Books">Books</a></li>
<li><a href="#Papers">Papers</a></li>
</ul>
</li>
<li><a href="#Lectures%201">Lectures 1</a>
<ul>
<li><a href="#Probability">Probability</a></li>
<li><a href="#Reading">Reading</a></li>
<li><a href="#Parameters">Parameters</a></li>
<li><a href="#PML%20book%20chapter%201">PML book chapter 1</a></li>
</ul>
</li>
<li><a href="#Lectures%202">Lectures 2</a>
<ul>
<li><a href="#10-601%20Decision%20Trees%20Part%201">10-601 Decision Trees Part 1</a></li>
<li><a href="#10-701%20Probability%2C%20MLE%20and%20MAP">10-701 Probability, MLE and MAP</a></li>
<li><a href="#CS4780%20Lecture%202%20KNN">CS4780 Lecture 2 KNN</a></li>
<li><a href="#Assignment%201">Assignment 1</a>
<ul>
<li><a href="#Assignment%201--Probability">Probability</a></li>
<li><a href="#Calculus">Calculus</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</nav>

<div id="outline-container-Courses" class="outline-2">
<h2 id="Courses">Courses</h2>
<div class="outline-text-2" id="text-Courses">
<p>
Let's start with machine learning. I'm going to do these 3 courses but you don't have to, they all cover the same thing (supervised learning) so if something doesn't make sense in one lecture, you can watch it presented differently in another course. Wildberger's <a href="https://www.youtube.com/playlist?list=PLIljB45xT85AMigTyprOuf__daeklnLse">playlist</a> of probability &amp; stats is a geometric explanation and all you need to start, watch it concurrently with the ML lectures. We will pick up all the prereqs as we do the assignments.
</p>

<ul class="org-ul">
<li>10-701 (Fa 2020) <a href="https://www.cs.cmu.edu/~epxing/Class/10701-20/schedule.html">Intro to Machine Learning</a> w/<a href="https://www.youtube.com/playlist?list=PLsWN0V-b507g7dbQTUvFkKZEqdHR5Fh4P">lectures</a></li>
<li>10-301/601 (Sp 2020) <a href="http://www.cs.cmu.edu/~mgormley/courses/10601/schedule.html">Intro to Machine Learning</a> w/<a href="https://www.youtube.com/playlist?list=PLpqQKYIU-snAPM89YPPwyQ9xdaiAdoouk">lectures</a></li>
<li>CS4780 (2018) <a href="https://www.youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS">ML for Intelligent Systems</a> w/<a href="http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/">lectures</a></li>
</ul>

<p>
The courses use C++ Python Java Octave but I'm using Julia simply because there exists excellent resources
</p>

<ul class="org-ul">
<li>MIT's <i>Introduction to Computational Thinking</i> <a href="https://computationalthinking.mit.edu/Spring21/">18.S191</a> (Sp 2020)
<ul class="org-ul">
<li>Amazing set of lectures that explain programming probabilities, the SVD (linear algebra)</li>
<li>ML w/Julia <a href="https://juliaacademy.com/p/introduction-to-machine-learning">tutorial</a> for using the AI libraries
<ul class="org-ul">
<li>Same prof has a <a href="https://mitmath.github.io/18337/">course</a> w/YouTube lectures on AI performance engineering</li>
</ul></li>
</ul></li>
</ul>


<p>
Some backups  
</p>
<pre class="example" id="org21ff14a">
Torrent for 10-701 lectures/assignments
magnet:?xt=urn:btih:40ea1c0bb1dbbef33e2f7ffb5df0106f03d50a3e&amp;dn=10701

Torrent for 10-301/601 lectures/assignments
magnet:?xt=urn:btih:2e1005d058b5f4c357d7338c85937aabdd91dcdc&amp;dn=10601
</pre>
</div>

<div id="outline-container-Books" class="outline-3">
<h3 id="Books">Books</h3>
<div class="outline-text-3" id="text-Books">
<p>
The courses do not follow a single book and recommend the following, many are free or try <a href="https://en.wikipedia.org/wiki/Library_Genesis">libgen</a>:
</p>

<ul class="org-ul">
<li><i>Machine Learning A Probabilistic Perspective</i> by Kevin Murphy
<ul class="org-ul">
<li>2021 <a href="https://probml.github.io/pml-book/book1.html">draft</a> is free &amp; includes Jupyter <a href="https://github.com/probml/pyprobml/tree/master/book1">notebooks</a> for each chapter</li>
</ul></li>
<li><i>Pattern Recognition and Machine Learning</i> by Bishop is free <a href="https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/">here</a></li>
<li><i>Machine Learning</i> by Tom Mitchell w/ <a href="http://www.cs.cmu.edu/%7Etom/NewChapters.html">new</a> chapters</li>
<li><i>The Elements of Statistical Learning</i> is <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">free</a></li>
</ul>

<p>
General modeling books   
</p>
<ul class="org-ul">
<li><i>Mathematical Modeling and Applied Calculus</i> reviewed <a href="https://www.maa.org/press/maa-reviews/mathematical-modeling-and-applied-calculus">here</a></li>
<li><i>Principles of Applied Statistics</i> by Cox/Donnelly reviewed <a href="http://bactra.org/reviews/cox-donnelly.html">here</a></li>
</ul>

<p>
Mathematics reference
</p>
<ul class="org-ul">
<li><i>Mathematics for Machine Learning</i> by Deisenroth/Faisal/Ong is <a href="https://mml-book.github.io/">free</a></li>
<li>Goodfellow's <i>Deep Learning</i> <a href="https://www.deeplearningbook.org/contents/prob.html">book</a> chapter on Probability &amp; Information Theory</li>
</ul>
</div>
</div>


<div id="outline-container-Papers" class="outline-3">
<h3 id="Papers">Papers</h3>
<div class="outline-text-3" id="text-Papers">
<ul class="org-ul">
<li>Depth First Learning builds <a href="https://www.depthfirstlearning.com/">curriculums</a> around papers to understand them</li>
<li>MIT's graduate section for 6.034 on interesting AI <a href="https://ai6034.mit.edu/wiki/index.php?title=6.S966:_A_Graduate_Section_for_6.034">papers</a></li>
</ul>

<blockquote>
<p>
"Recent astonishing progress in "machine learning" has eclipsed much of the traditional work on symbolic thinking. But problems remain: the systems that result from work on machine learning research have no concept of meaning&#x2013;the "words" do not have referents outside of the ways in which they are used. Such systems may perform well on many tasks but they do not smoothly interface with systems that are organized around modeling the world, which is probably essential to solving really deep problems of common sense and science." - <a href="https://ai6034.mit.edu/wiki/index.php?title=6.S966:_A_Graduate_Section_for_6.034">Gerald Sussman</a>
</p>
</blockquote>
</div>
</div>
</div>


<div id="outline-container-Lectures%201" class="outline-2">
<h2 id="Lectures%201">Lectures 1</h2>
<div class="outline-text-2" id="text-Lectures%201">
<p>
Let's audit the first intro lectures for CS4780, 10-701, and 10-601.
</p>


<p>
10-601 primarily talked about history of ML like expert systems and why they failed. We learn the brief difference between regression output, probability output and classification. Most of this lecture is course logistics you can skip.
</p>

<p>
10-701 when you get past course logistics talks about how machine learning constantly changes, there was a breakthrough in neural networks recently so now it's all about neural networks, we used to heavily use kernel methods and now do not. We could use convex optimization in the past and now things are much more non-convex.  
</p>

<p>
CS4780 the <a href="https://youtu.be/MrLPzBxG95I">intro</a> lecture jumps into supervised learning setup, explaining the math notation. His high level explanation of how machine learning works: insert data, insert expected output, generate program. At 32:00 the supervised learning intro begins. Lecture notes are <a href="http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/">here</a>. He uses arrows on top of the notation to denote vectors. Look up what <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">iid</a> is: "all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples". \(D = \{ (x_{i}, y_{i})....(x_{n},y_{n}) \}\) is a subset of XY meaning a cartesian product of every single combination of X (features) with Y (labels), like the cartesian coordinates in a x and y graph, you can combine any one of them to form a point: (0,1) or (100,-1), so using his patient data vector example: Y would be binary 0 or 1 to represent yes or no, X is the feature set of a patient and D is the total training data set of all possible combinations of feature set with yes and no.
</p>

<p>
You can learn basic set theory by typing it into <a href="https://www.youtube.com/watch?v=59WX2V7Vjgg">YouTube</a> and vectors are covered in the reading, or the book <i>Mathematical Modeling and Applied Calculus</i> from the beginning of these notes.
</p>
</div>

<div id="outline-container-Probability" class="outline-3">
<h3 id="Probability">Probability</h3>
<div class="outline-text-3" id="text-Probability">
<p>
Wildberger's probability lecture <a href="https://youtu.be/siXj4hkUIp8">ProbStats5: Random variables</a> will explain notation like \(X^{-1}(x_i)\). A random variable is a function over the sample space that returns a real number and he geometrically demonstrates this function. Expected value E(x) is demonstrated as the mean, variance Var(x) is demonstrated as the distance between probabilities which define your distribution, and standard derivation SD(x) turns the variance from quadratic to linear. Probability is covered extensively by all the books the courses recommend but this is a nice lecture to visualize a random variable and all the other Wildberger lectures are similar first presenting a visualization of the theory then examples.
</p>
</div>
</div>

<div id="outline-container-Reading" class="outline-3">
<h3 id="Reading">Reading</h3>
<div class="outline-text-3" id="text-Reading">
<p>
Let's skim chapter 8.1 <i>When Models Meet Data</i> of the <a href="https://mml-book.github.io/">mml book</a>. We take tabular data, represent it as a vector with N rows (samples) and D columns (features). Once we have a vector representation we can use concepts from linear alegbra to manipulate it, and to compare it to another vector by constructing a geometry we can run optimizations on. We read about different ways to construct new features and that increasingly neural networks are used to learn new features which was mentioned in the first lecture of 10-701. There are two schools: machine learning where the predictor is a function or a probability model, we will mainly be covering the probability model in 10-601/701. In 8.1.4 the goal is to construct a model so that it's parameters will perform well on new unseen data.
</p>
</div>
</div>

<div id="outline-container-Parameters" class="outline-3">
<h3 id="Parameters">Parameters</h3>
<div class="outline-text-3" id="text-Parameters">
<p>
What is a parameter? In the chapter on vector calculus, these are described as 'an optimization problem: find parameters that control how well a model explains the data' and there is a concrete example of a curve model of training data, where at each observation we want to find parameters that explain the observations basically finding a signal in noise. Skimming through chapter 7 of <i>Principles of Applied Statistics</i> on criteria for parameters: "we aim to summarize the aspects of interest by parameters, preferably small in number and formally defined as properties of the probability model". There's some examples here like a pipe with water flowing through it, if you take measurements what is the minimum amount of measurements to include as parameters to your water pipe model that best describes the observations like velocity, the area inside the pipe, the viscosity of the water (feature set). The choice of regression model is explained as the search for a model with as few explanatory variables as necessary to give an adequate empirical fit to the data where overfitting means your model too closely resembles the sample and can't be generalized. Summary: parameters are what you specify some object with, like if you were to specify a cat object, what are the minimum amount of cat characteristics you could use to distinguish the object as a cat from other objects. 
</p>
</div>
</div>

<div id="outline-container-PML%20book%20chapter%201" class="outline-3">
<h3 id="PML%20book%20chapter%201">PML book chapter 1</h3>
<div class="outline-text-3" id="text-PML%20book%20chapter%201">
<p>
Let's also skim the first chapter in the 2021 <a href="https://probml.github.io/pml-book/book1.html">book</a> <i>Probabilistic Machine Learning</i>. Each chapter also has a colab notebook you can run <a href="https://github.com/probml/pyprobml/tree/master/book1">here</a> in Python. This is a terse survey of what we are about to take in detail and much of this is what we just saw in CS4780's first lecture. All of these stats functions exist as libraries in programming language of choice.
</p>

<p>
Reading 1.2 <i>Supervised Learning</i>, if you took the compsci <a href="https://learnaifromscratch.github.io/software.html">workshop</a> you know exactly what tabular data and fixed-size feature representation is as it was the first assignment we did turning documents into vectors and <a href="https://www.khanacademy.org/computing/computer-programming/programming-natural-simulations/programming-vectors/a/vector-magnitude-normalization">normalizing</a>. The 'design matrix' example in Murphy's book notes that this is what <i>big data</i> means because N (150 rows) is greater than D (4 columns), the number of features. Figure 1.4 we have simple if-else statements represented as a decision tree, value = [0, 49, 5] for versicolor means our decision tree only correctly labelled 49 versicolors and mislabelled 5 virginica. 
</p>

<p>
The decision rule function f(x;theta), I read it as x is the input, consuming that entire table of data for the flowers as a vector, and theta (the parameters) are features and their threshold to define the decision function like feature 'petal-length' &lt;= 2.45.
</p>

<p>
1.2.1.4 the goal of supervised learning is coming up with a classification model like the decision tree, to reliably predict labels for any new data. The loss function notation is explained in the appendix. Reading from left to right, Loss-Fun(params) is defined as the average of the sum of all outputs from a binary indicator function so summing 0 and 1's then dividing by the total number of indices of input vector x to get the average. The binary indicator function is a boolean comparing the target y (the known label) to the predicted label output of f(x;params) as you iterate the row indices of that table of data. This is explained in <i>Mathematics for Machine Learning</i> in chapter 8.3.1, or search the entire pdf for negative log-likelihood. According to that book the interpretation should be if you vary the parameters on fixed data (our vector input of flower features) this function tells us how likely a particular setting of the parameters is to explain the observations in the input vector. Example 8.5 of that book shows this as a least-squares problem and both log(), exp() and least squares are explained in the <i>Mathematical Modeling and Applied Calculus</i> text. 
</p>


<p>
In the following function the hat notation on top of the y means estimated/predicted value of y. That entire function is then further reduced to the notation \(l_{01}(y,\hat{y})\) to indicate zero-one loss of true label vs predicted label. The last function empirical risk minimization, argmin is defined <a href="https://math.stackexchange.com/a/2157524">here</a> meaning what x input to f(x) results in the global minimum (lowest y value) so imagine that you are feeding a decision rule function with numerous different thresholds/features as parameters, that argmin function should return the parameters with the least average mislabeling on the training set.
</p>

<p>
Some conditional probability introduced here that are in the above Wildberger lectures. Equation 1.7 if you look at the appendix means the 'parametric distributions' or probability that y = label given random variable x and parameters, and it's equated to more terse notation of \(f_{c}(x;\theta)\). That p() function returns a number from 0 to 1, in this example 3 ordered elements of a list that represent the 3 labels, with the probability of them being labelled setosa 0/54 so highly unlikley to happen, and the probability of being labelled versicolor 49/54 or 0.9 so very high probability but there is still uncertainty. This probability function output is then input to NLL() and if you were to plot a negative log() function in R/Python/Julia, you would see that when given a higher number like 1, the graph is very close to the x-axis (minimum loss) whereas if probability is closer to 0, the graph shoots up to infinity on the y-axis meaning a lot of loss. MLE is explained in the <i>Mathematics for ML</i> book in chapter 8.3.1 "MLE gives us the most likely parameter for the set of data" and there's lectures regarding MLE in 10-601/701.
</p>
</div>
</div>
</div>

<div id="outline-container-Lectures%202" class="outline-2">
<h2 id="Lectures%202">Lectures 2</h2>
<div class="outline-text-2" id="text-Lectures%202">
</div>
<div id="outline-container-10-601%20Decision%20Trees%20Part%201" class="outline-3">
<h3 id="10-601%20Decision%20Trees%20Part%201">10-601 Decision Trees Part 1</h3>
<div class="outline-text-3" id="text-10-601%20Decision%20Trees%20Part%201">
<p>
Watching 10-601 second lecture <i>Decision Trees (Part 1)</i>. There's secret <a href="https://1drv.ms/o/s!Aqk9RupCw3gqhnEVySsGVwiAwMI6">notes</a> and also a math notation <a href="http://www.cs.cmu.edu/~mgormley/courses/10601-s20/slides/10601-notation.pdf">crib sheet</a> that is extremely helpful. 
</p>

<p>
So far, if you took CS19 from the compsci workshop then all of this is familiar ie: table of data, function type notation. If it seems unclear, the c*(x) function is whatever generated the training data, so a doctor providing their medical diagnosis history over x months with hundreds of patients and c*(x) is the function in the doctor's brain that generated the decisions in the training data, and h(x) is what we want to build that best approximates c*(x).
</p>

<p>
@37:00 or so we get introduced to loss functions we already read about in the first chapter in the 2021 <a href="https://probml.github.io/pml-book/book1.html">book</a> <i>Probabilistic Machine Learning</i> except he's using the compact abstract notation. We have to write a decision stump algorithm in the homework, write down it's spec he gives in lecture since it's not in the assignment writeup for some reason. The website schedule for the course has a link to <a href="http://ciml.info/dl/v0_99/ciml-v0_99-ch01.pdf">this</a> reading which is exactly what we already read though they explain the notation better.
</p>
</div>
</div>

<div id="outline-container-10-701%20Probability%2C%20MLE%20and%20MAP" class="outline-3">
<h3 id="10-701%20Probability%2C%20MLE%20and%20MAP">10-701 Probability, MLE and MAP</h3>
<div class="outline-text-3" id="text-10-701%20Probability%2C%20MLE%20and%20MAP">
<p>
Watching <a href="https://youtu.be/BqoKDBHtrno">lecture</a> 2 a helpful review of probability, you may want to do the 10-301 hw first before watching. 'In machine learning, random variables are seldom independent'. He adds that in 10-701 we will (incorrectly) assume they are independent anyway as it's easier to work with independent random variables and for the purposes of machine learning the outcome is fine. Recall from the Wildberger lectures, that a probability distribution is defined by it's mean or expected value, which is the 'center of the mass', and it's variance which is how far away the rest of the probabilites are from each other, this gives you that normal distribution curve or some other distribution. The prof in this lecture remarks we will often assume the distribution and solely be concerned with estimating the parameters. Throughout he remarks that often in probability definitions the thing dividing the top is a normalization function to get the probabilities to sum to 1. For example conditional probability P(A|B) = P(A and B)/P(B). Make a toy set: A = {1,2,3} and B = {3,2} then we have P({2,3})/P({2,3}) or 1.
</p>

<p>
Bayes rule and everything discussed up to 25mins, plus continuous probability is in the Wildberger lectures. There's an anecdote about false positive rates, and @46:00 he talks about the bayesian approach (MAP) for estimating parameters which he says won't be used in the class. This is all in the Murphy and Mitchell texts.
</p>

<p>
@1:04 we get our first algorithm MLE. A density estimator learns a mapping from a set of attributes to a probability. The likelihood of the data given the model: P(set of observations|model) = product of each observation conditioned on the parameters. Model is defined as a collection of parameters, since however you decide to parameterize an object to define it that becomes it's model. We will shortly see MLP or maximizing of a function in the 10-301 homework q = (number of observed heads)/total rolls.
</p>

<p>
The prof teaching this is the head of CMU's systems bio group and maintains an interesting page on <a href="https://algorithmsinnature.org/">algorithms used by nature</a>. The second half of 10-701 is taught by prof Xing who somehow has 2 PhDs and is the new <a href="https://mbzuai.ac.ae/about">president</a> of MBZUAI the world's first AI only university, which offers fully paid MSc and PhD degrees. I would imagine if you took his CMU course and can understand it, and you already had a bachelors they would likely grant you admission.  
</p>
</div>
</div>

<div id="outline-container-CS4780%20Lecture%202%20KNN" class="outline-3">
<h3 id="CS4780%20Lecture%202%20KNN">CS4780 Lecture 2 KNN</h3>
<div class="outline-text-3" id="text-CS4780%20Lecture%202%20KNN">
<p>
Watching <a href="http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html">lecture</a> k-nearest neighbors, which is only the last 15 minutes of the lecture the rest is practical advice how you should organize your learning, validation and testing data, like seperating temporal (data with a sense of time) data so your learning algorithm 'can't look at future data' to avoid overfitting meaning your model no longer generalizes, you're just fitting it for the test data. He talks about why you can't have a single algorithm to do everything, this is worth watching and not very technical. The distance equation is in the notes, fill in some values to see what they are rational exponents are defined in Tao's <i>Analysis I</i> book Definition 5.6.7 \(x^\frac{a}{b}\) is \((x^\frac{1}{b})^a\) or if \((thing) = x^\frac{1}{n}\) then \((thing)^n = x\), it's just the square root. You can look up Euclidean distance though you've probably already seen it, personally I've never seen Manhattan distance instead I learned it as Hamming distance which is the same thing. The point of this algorithm as per the student who asks the question "why are we doing this" is a generalized distance algorithm where you can pick the model you want for k-nearest neighbors. 
</p>
</div>
</div>

<div id="outline-container-Assignment%201" class="outline-3">
<h3 id="Assignment%201">Assignment 1</h3>
<div class="outline-text-3" id="text-Assignment%201">
<p>
Try the 10-301 <a href="https://gitlab.com/mathfromtheverybeginning/homework-10301/-/tree/master/hw1">homework</a>, it has an autograder for some languages but you don't need it. All these classifier functions like decision stumps already exist in highly optimized <a href="https://github.com/bensadeghi/DecisionTree.jl/blob/master/README.md">libraries</a> so I implemented the assignment using a few filters in <a href="https://code.pyret.org/">Pyret</a>. You can import the testing and training data as google sheets, convert them to tables then extract and filter columns. Anything in the homework files ending in .labels is the solution which you can literally cut and paste into google sheets, import as a table in Pyret and then extract as a list to run test cases to see if you pass though you won't need to this is a simple assignment. 
</p>

<pre class="example" id="org31620ea">
include gdrive-sheets

ssid = "1w53u8p8eiyQ5efww54yz9b3WHcFgPMQe-Em0oLG5AJ0"
politicians-train =
  load-table: Anti_satellite_test_ban, Aid_to_nicaraguan_contras, Mx_missile, Immigration, Superfund_right_to_sue, Duty_free_exports,	Export_south_africa, Party
    source: load-spreadsheet(ssid).sheet-by-name("politicians_train", true)
  end

ssid2 = "1baH3PUQ852FSHZJ063v-ozi0wFARXEHIKIPgHCyqVVY"
politicians-test =
  load-table: Anti_satellite_test_ban, Aid_to_nicaraguan_contras, Mx_missile, Immigration, Superfund_right_to_sue, Duty_free_exports,	Export_south_africa, Party
    source: load-spreadsheet(ssid2).sheet-by-name("politicians_test", true)
  end
</pre>
</div>

<div id="outline-container-Assignment%201--Probability" class="outline-4">
<h4 id="Assignment%201--Probability">Probability</h4>
<div class="outline-text-4" id="text-Assignment%201--Probability">
<p>
In the hw1 folder if you either downloaded it from the torrent or got it from <a href="https://gitlab.com/mathfromtheverybeginning/homework-10301/-/tree/master/hw1">here</a> there's two pdf files, the file labelled with 'tex' means a student filled in the solutions and generated a new pdf with LaTeX but you shouldn't automatically trust these solutions. For example here is a student quietly breaking academic protocol asking the <a href="https://math.stackexchange.com/questions/3814831/maximizing-probability-of-a-sequence-of-coin-tosses-for-a-biased-coin">same</a> question as problem 2. Are they right about question 1 though, that it's 81/1024? It asked what is the probability of observing <i>any combination</i> and the answer to question 1 is <a href="https://youtu.be/fyOdJ34iMpU">here</a> which is: \(\binom{n}k \cdot p^k(1-p)^{n-k}\) or \(\binom{5}4 \cdot 0.75^4(0.25)^1\) = 405/1024. In the response is a link to this <a href="https://www.wolframalpha.com/input/?i=plot+x%5E4%281-x%29%2C+for+x+in+%280..1%29">graph</a> we can see the maximum x input value for that function is 0.8, or 4/5 or <a href="https://math.stackexchange.com/questions/636539/intuitive-way-to-arrive-at-the-maximizing-argument-for-the-binomial-probability">this</a> answer. Global minimum/maximum is covered in the book <i>Mathematical Modelling and Applied Calculus</i> in Chapter 5: Optimization.  
</p>

<p>
Question 3, 4 are in the Wildberger lectures, question 5 is this <a href="https://youtu.be/ERC9H-8kBHs?t=1296">tree</a> example he draws for total probability, P(R)P(W|R) + P(NR)P(W|NR) or 0.4*0.8 + 0.6*0.2 = 0.44
</p>

<p>
Question 6, P(X=1|Y=1) the set of A where X=1 is {(1,0), (1,1)} and B where Y=1 is {(0,1), (1,1)}. We are interested in A intersect B, divided by B (as per Wildberger lectures on Bayes rule). A intersect B is (1,1) and B is {(0,1), (1,1)} so 0.3/0.5 or 0.6. The probability P(Y=0) is 0.1 + 0.4 
</p>

<p>
8, 9 and 10 is in the Wildberger <a href="https://youtu.be/pdaNVZQ9974?t=2548">lectures</a> for example at time index 42:30 <i>Transformation Formulas</i>, like E[6X]. Since E[x] is defined as a sum, multiplying by some scalar means you can factor it out ie: 6(E[X]) = E[6X]. Var[2x + 3] is fully explained in the same probability lecture you ignore addition and just square the scalar. Recall the variance is a sum so every input of x_i will add 3, shifting everything over by 3 which doesn't matter the distance between each x_i remains the same, as they're all moved over 3.
</p>

<p>
Question 11 and 14 solution is <a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading7a.pdf">here</a> in MIT OCW lecture notes, for #14 to determine b_3 conditioned on c_1, you take 0.35 and divide by the total marginal probability of c_1 which is .02 + .03 + .35 or .35/.40
</p>

<p>
Goodfellow's <i>Deep Learning</i> <a href="https://www.deeplearningbook.org/contents/prob.html">book</a> (doesn't work on some browsers, try Chrome or use library genesis) gives the answers for the rest of this probability homework, question 15 the expectation of Y is the sum of (values * probability) which is 1/n * y. or y/n and 16 is clearly the first or second option, given that expected value(mean) of a bernoulli random variable p is p. Searching wikipedia for Bernoulli entropy I find the answer is A, though we will do log(), exp() in the next part of the homework when we learn calculus.
</p>
</div>
</div>

<div id="outline-container-Calculus" class="outline-4">
<h4 id="Calculus">Calculus</h4>
<div class="outline-text-4" id="text-Calculus">
<p>
These problem sets, if you wanted you could use some library built-ins and eval them, for example the first question is a trivial Wolfram Alpha/Matlab/Octave/Julia calculation but of course we're going to learn calculus anyway.
</p>

<p>
What is exp() and ln() in both question 1 and 2, plus the gaussian distribution we saw earlier? TODO
</p>







<hr>
<p>
<a href="./index.html">Home</a>
</p>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
