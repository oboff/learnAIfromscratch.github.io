<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2021-01-28 Thu 10:15 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>An Artificial Intelligence Curriculum</title>
<meta name="generator" content="Org mode">
<meta name="author" content="jbh">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  body {
  max-width: 40rem;
  padding: 1rem;
  margin: auto;
  }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<style> body {background-color: #fafad2} </style>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<header>
<h1 class="title">An Artificial Intelligence Curriculum</h1>
</header><nav id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#Intro">Intro</a>
<ul>
<li><a href="#Materials">Materials</a></li>
</ul>
</li>
<li><a href="#Start%20here">Start here</a>
<ul>
<li><a href="#Probability">Probability</a>
<ul>
<li><a href="#Lecture%201%2015-251">Lecture 1 15-251</a></li>
<li><a href="#Lecture%202%2015-251">Lecture 2 15-251</a></li>
<li><a href="#Probability%20%26%20Markets">Probability &amp; Markets</a></li>
<li><a href="#Exercises">Exercises</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</nav>

<div id="outline-container-Intro" class="outline-2">
<h2 id="Intro">Intro</h2>
<div class="outline-text-2" id="text-Intro">
<p>
This is a self-directed workshop to learn machine learning, neural networks, autonomous agents and whatever else we can find. Like the other two workshops on <a href="https://learnaifromscratch.github.io/math.html">math</a> and <a href="https://learnaifromscratch.github.io/software.html">computer science</a> this is a course built around courses. Self-directed means choose your own curriculum. I'll audit resources and you can decide to take it yourself or not. My interest will be: learning some of the popular current models and methods for learning, how to <a href="https://mitmath.github.io/18337/">optimize</a> these models and deploy them, and <a href="https://ai6034.mit.edu/wiki/index.php?title=6.S966:_A_Graduate_Section_for_6.034#Prospectus">criticism</a> of the current state of AI to see if there's something else we should be trying.
</p>

<p>
I will go through some of the lectures and papers, and some of the prereqs like picking up probability, matrix manipulations, calculus. You can then figure the rest out yourself easily really the only issue is getting to a level where all these abstract models and notation make sense then you can coast to the finish line. 
</p>
</div>

<div id="outline-container-Materials" class="outline-3">
<h3 id="Materials">Materials</h3>
<div class="outline-text-3" id="text-Materials">
<p>
As I go I will choose various materials from the following list that will probably grow over time:
</p>

<ul class="org-ul">
<li><a href="https://www.depthfirstlearning.com/">Depth First Learning</a> are curriculums built around a single paper. You can learn exactly how AlphaGo and other game solving techniques work, neural networks, different training methods. Many of these paper curriculums were funded by Jane Street Capital so have applications in quantitative finance.</li>
<li>10-701 (Fall 2020) <a href="https://www.cs.cmu.edu/~epxing/Class/10701-20/schedule.html">Intro to Machine Learning</a> has YouTube recorded lectures and is CMU's graduate course in machine learning. The difference between CMU ML intro courses is <a href="https://docs.google.com/document/d/1Y0Jx_tcINWQrWJx31WGEQSsUs059OUMmPIVSeyxNdeM/edit">here</a>. It's also a survey of methods by current researchers so they will talk about the latest models and methods, what doesn't work for what.</li>
<li>10-301/601 (Spring 2020) <a href="http://www.cs.cmu.edu/~mgormley/courses/10601/schedule.html">Intro to Machine Learning</a> also has YouTube <a href="https://www.youtube.com/playlist?list=PLpqQKYIU-snAPM89YPPwyQ9xdaiAdoouk">recorded lectures</a> with highly polished slides and lectures.</li>
<li>15-388 (2019) <a href="https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22618ea253-ca45-4b14-9f1d-aab501543bd2%22">Practical Data Science</a> another CMU course has good lectures to use as recitations for prereqs like linear algebra or probability, and the <a href="http://www.datasciencecourse.org/lectures/">notes</a> are in interactive jupyter notebooks you can view or run yourself as you read them.</li>
<li>CS4780 (2018) <a href="https://www.youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS">Machine Learning for Intelligent Systems</a> Cornell's intro ML class has the best instructor I find in terms of explaining the math notation, keeping on point (no long offtopic anecdotes), and has the best <a href="http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/">notes</a>. This class only covers supervised learning which has overlap with unsupervised and reinforcement learning.</li>
<li>Many book chapters, papers, guides all suggested by these courses&#x2026; primarily the Deep Learning book by Goodfellow, Mathematics for Machine Learning, and Strang's new Linear Algebra w/Deep Learning.</li>
<li>A <a href="https://computationalthinking.mit.edu/Fall20/">course</a> on programming models so we can do things like import any Python or R library we want and use it, and learn how to optimize our programs when deploying them instead of every other ML curriculum that stops with the modeling and tells you nothing about GPUs or parallel deployment. You can of course use any language you want, even common <a href="https://github.com/numcl/numcl">lisp</a>.</li>
</ul>

<p>
Maybe try some AI competitions, like <a href="https://halite.io/">Halite</a>. 
</p>
</div>
</div>
</div>

<div id="outline-container-Start%20here" class="outline-2">
<h2 id="Start%20here">Start here</h2>
<div class="outline-text-2" id="text-Start%20here">
<p>
Let's audit the first intro lecture for CS4780, 10-701, and 10-601. The learning goals of 10-601 are <a href="http://www.cs.cmu.edu/~mgormley/courses/10601/slides/10601-objectives.pdf">here</a>, it's first lecture primarily talked about history of ML like expert systems and why they failed. We learn the brief difference between regression output, probability output and classification. Most of this lecture and 10-701 are course logistics, you can watch both in under 30mins skipping the logistics.
</p>

<p>
10-701 when you get past course logistics talks about how ML methods change often, there was a breakthrough in neural networks recently so now it's all about neural networks, we used to heavily use kernel methods and now not as much. We could use convex optimization back then and now things are much more non-convex.  
</p>

<p>
Meanwhile in CS4780 the <a href="https://youtu.be/MrLPzBxG95I">intro</a> lecture jumps into supervised learning setup, explaining the math notation. His high level explanation of how machine learning works: insert data, insert expected output, generate program. At 32:00 the supervised learning intro begins. Lecture notes are <a href="http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/">here</a>. He uses arrows on top of the notation to denote vectors. Look up what <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">iid</a> is: "all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples". \(D = \{ (x_{i}, y_{i})....(x_{n},y_{n}) \}\) is a subset of XY meaning a cartesian product of every single combination of X (features) with Y (labels), like the cartesian coordinates in a x and y graph, you can combine any one of them to form a point: (0,1) or (100,-1). So using his patient data vector example, Y would be binary 0 or 1 to represent yes or no. X is all the feature set of a patient and D is the total training data set of all possible combinations of feature set with yes and no.
</p>

<p>
You can learn basic set theory by typing it into <a href="https://www.youtube.com/watch?v=59WX2V7Vjgg">YouTube</a> or going through the math workshop, which also covers what a vector is.
</p>
</div>
<div id="outline-container-Probability" class="outline-3">
<h3 id="Probability">Probability</h3>
<div class="outline-text-3" id="text-Probability">
<blockquote>
<p>
"Probability = analyzing random code"
</p>
</blockquote>

<p>
We will start with <a href="https://youtu.be/gtNNEPuyVXo">Probability 1</a> and some of <a href="https://youtu.be/p7fqy9FLA8A">Probability 2</a> lectures taught by Ryan O'Donnell, the full notes with exercises are <a href="https://www.anilada.com/courses/15251f17/www/notes/notes_all_but_sols.pdf">here</a> to practice after, and the <a href="https://www.anilada.com/courses/15251f17/www/notes/notes_exs_sols.pdf">solutions</a>. We will use that understanding to level up to Cozma Shalizi's <a href="http://bactra.org/prob-notes/srl.pdf">notes</a> on probability and statistics, watch some Wildberger stats <a href="https://www.youtube.com/playlist?list=PLIljB45xT85AMigTyprOuf__daeklnLse">lectures</a>, finally trying the chapter on probability from the math for ML <a href="https://mml-book.github.io/">book</a> and Knuth's <a href="https://libgen.is/book/index.php?md5=930D5B423E3648214E3600A761A11A11">draft chapter</a> on probability from <i>Mathematical Preliminaries Redux</i> because there is a lot of confusing notation, so practice with this framework will help us understand it.  
</p>
</div>

<div id="outline-container-Lecture%201%2015-251" class="outline-4">
<h4 id="Lecture%201%2015-251">Lecture 1 15-251</h4>
<div class="outline-text-4" id="text-Lecture%201%2015-251">
<p>
I'm watching the 2013 15-251 YouTube lecture and the 2015 Panopto version <a href="https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6a3880a8-fbf8-4c10-8ebb-a7ca8360a0ba&amp;start=undefined">here</a> with unobstructed slides, and the lecture clarifies things better than the 2013 lecture, though the notes for the course cover everything. In the 2013 YouTube lecture we are interrupted by an ancient <a href="https://youtu.be/DFB4Iw7BHoU">meme</a>. The probability tree, if you haven't figured it out you start with 1/2 up top and now each outcome must add up to 1, so RandInt(3) left side is 3/6 or 1/2, which when combined with Bernoulli(1/2) is 1. The term <i>notation overload</i> means multiple definitions for the same notation, hoping you derive from context which definition the notation is using.
</p>

<p>
The 2015 Panopto lecture at 27:05 explains the "I will roll two dice 24 times. I win if I get a double-1s" better than the 2013 lecture. There are 36 outcomes each time he rolls 2 dice ie: outcome: 1,1 or 1,2 or 1,3 or 1,4 or 1,5 or 1,6. If you draw a tree this will end up being 36 total outcomes with each roll since 6x6. Remove the single outcome where it comes up 1,1, so 35/36 outcomes per roll and there is 24 rolls. Subtract that from the total probability of 1 in other words the \(Pr[W^{c}]\) compliment definition. The silver and gold coin problem you will get right away as 2/3 probability if you've ever seen those Monty Hall probability <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/video-lectures/lecture-18-probability-introduction/">lectures</a> from MIT's 6.042J except here I find it's explained better, later in the lecture he explains why he doesn't teach the Monty Hall example.  
</p>

<p>
2015 lecture for independence of multiple events, the big union or 'and' notation \(\bigcup\) or the \(\prod\) product notation means as you iterate through a sequence like \(Pr[A_1]\) to \(Pr[A_5]\), you insert that operation between each value ie for intersection: \(Pr[A_1] \cap Pr[A_2] \cap Pr[A_3]\) or \(Pr[A_1] \cdot Pr[A_2] \cdot.. Pr[A_i]\).
</p>
</div>
</div>

<div id="outline-container-Lecture%202%2015-251" class="outline-4">
<h4 id="Lecture%202%2015-251">Lecture 2 15-251</h4>
<div class="outline-text-4" id="text-Lecture%202%2015-251">
<p>
Random variables are explained as whatever the value that variable has at the end of computation. Random variables are a function from the sample space to the real numbers. Any function you obtain from an existing random variable is another random variable as described in this MIT <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041sc-probabilistic-systems-analysis-and-applied-probability-fall-2013/unit-i/lecture-5/">lecture</a> you have Omega = set of all students, a function from one student in the Omega set to their height in inches is a random variable (sample set to real number). A new function from their height in inches to their height in cms is another random variable. Tao's discrete probability foundations <a href="https://terrytao.wordpress.com/2015/09/29/275a-notes-0-foundations-of-probability-theory/">here</a> explain this concept as a way to extend the sample space by creating a new sample space with new probabilities, much like how you would change basis in linear algebra.    
</p>

<p>
The YouTube version is the same as the Panopto 2015 <a href="https://scs.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=d4ef6eb2-870e-4526-bdd8-79654572583a&amp;start=undefined">version</a>. Linearity of Expectation proofs he's just using distributive property to move stuff around, E[aX + b] is the same as (a(E[X]) + b) because these are linear/scaling operations. Note around 35:00 where he makes a comment about understanding the types: an event space can't be summed because it's a set, the indicator can be summed because it is 0 or 1, type real number. The last 10 minutes or so of this lecture you can optionally watch, he uses a graph coloring problem and talks about satisfiability, both explained in previous lectures if you're interested.
</p>
</div>
</div>

<div id="outline-container-Probability%20%26%20Markets" class="outline-4">
<h4 id="Probability%20%26%20Markets">Probability &amp; Markets</h4>
<div class="outline-text-4" id="text-Probability%20%26%20Markets">
<p>
It may be helpful to see all these abstractions with concrete examples in the real world. This <a href="https://www.janestreet.com/probability-markets/">guide</a> here is for people applying to Jane Street as a quantitative trader which explains all these random modeling concepts in a different view that may make more sense. This is how you learn, seeing the same highly abstract model in multiple contexts to learn it's behavior, then doing exercises to confirm you understand the model. This pdf has confidence intervals (confidence regions) which we will <a href="http://bactra.org/prob-notes/srl.pdf">read</a> about soon.    
</p>
</div>
</div>


<div id="outline-container-Exercises" class="outline-4">
<h4 id="Exercises">Exercises</h4>
<div class="outline-text-4" id="text-Exercises">
<p>
Let's try some of the exercises <a href="https://www.anilada.com/courses/15251f17/www/notes/notes_all_but_sols.pdf">here</a>, then compare to the <a href="https://www.anilada.com/courses/15251f17/www/notes/notes_exs_sols.pdf">solutions</a>. If you get stuck look at Wildberger's <a href="https://www.youtube.com/playlist?list=PLIljB45xT85AMigTyprOuf__daeklnLse">lectures</a> he walks through many of these proofs with visual diagrams. I'm also reading Knuth's <a href="https://libgen.is/book/index.php?md5=930D5B423E3648214E3600A761A11A11">chapter</a> on probability as I go through these. Try to explain everything to yourself like I've done here:
</p>

<details id="org20dff33">
<p>

</p>

<p>
15.1 A discrete probability space, Omega or \(\Omega\), is modeled as a tuple: (Omega, Pr[]). A tuple is another name for an ordered list, ie: a data structure in math. This tuple is a framework to model non-deterministic statements, meaning they have indeterminate state as explained by Terence Tao <a href="https://terrytao.wordpress.com/2015/09/29/275a-notes-0-foundations-of-probability-theory/">here</a>. Omega is type set, called a sample space, with Pr[] being type function that maps a real number between 0 and 1 to everything in the set Omega. The total sum of the Omega sample space is 1 as it would make no sense for our probability model to have negative probability, and make no sense to have probability greater than one: either the event will surely happen with probability 1, or not happen with probability 0, or possibly happen with probability from 0 to 1.       
</p>

<p>
Exercise 15.4, how do we model rolling a 6-sided die using 15.1 definition. The sample space = {1,2,3,4,5,6}, and Pr[] function has the property that every probability in the sample space must add up to one, so the leaves of that tree would be probability 1/6 each and Pr[roll] = 1/6. Two dice the sample space is 6x6 with 36 total leaves as possible outcomes, each leaf with probabilty 1/36. The coin and dice example, sample space = {H,T} x {1,2,3,4,5,6} in my head I see a tree with Heads or Tails branches with 1/2 probability, and attached to each branch is the outcome of a 1/6 dice roll, so Pr[Coin,Dice] is 1/2 x 1/6 or 1/12 which makes sense since each branch will have 6 outcomes, and there's two branches for H or T, and all 12 outcomes must add up to 1. Here's the same example with a 3 sided die:
</p>

<pre class="example" id="orgf3efedf">
       Bernoulli(1/2)
        /          \ 
  RandInt(3)     RandInt(3) 	
     /  \          /  \ 
(H,1),(H,2)(H,3) (T,1),(T,2),(T,3)
</pre>

<p>
In this example it's 1/2 x 1/3, or 1/6 probability in each leaf, which again makes sense as there are 6 leaves or outcomes x 1/6 = 1. 
</p>


<p>
Def 15.7 Event, a new set is introduced to model events. Any subset of our probability space, or sample space, is called an event. The power set of Omega is the set of all events, that is if Omega = {H,T} then the power set of all events is {{empty},{T},{H},{H,T}} so total 4 elements in the power set or \(2^n\) where n = size of Omega, otherwise known as the cardinality of set Omega. Notice we have 2 probability = 0 in that power set, as {H,T} is an impossible state (assuming you don't count a coin landing on it's side) and the empty set are both Pr = 0. An event is type set, so if you flipped the coin then the event that it lands on Tails is the set E = {T}, and Pr[E] is 1/2, as Pr[E] is defined here to be a function that returns the sum of all outcomes in set E. Knuth describes this event set as a proposition. Everytime you think of a proposition like "the flip will be Tails" there is a subset of only {T} or "the dice roll will be a 6" you get an event set E = {6}. In Knuth's writeup he makes this more clear using notation to show an event is the sum of probabilities of all elements in Omega that are also in set E. 
</p>

<p>
Def 15.8 we have two 6 sided dice so our event space is {1,2,3,4,5,6} x {1,2,3,4,5,6} or 36 possible outcomes. The proposition 'I will roll double numbers' is the event space E = {(1,1),(2,2),(3,3),(4,4),(5,5),(6,6)} and Pr[E] = 6/36 or 1/6 probability of that proposition occuring. A 3 sided die, where we roll a d sided die after, is E = {1,2,3} x {d} and Pr[E] is 1/3 x 1/d or 1/3d. How do we figure out 'the second roll is a 2'? The first roll must be &gt; 1 in order to be able to roll a 2 sided or more die after. This creates the subset E = {(2,2),(3,2)} as those are our only possible outcomes since 'second roll is a 2'. Def 15.7 the probability of Pr[E] is the sum of all probabilities in E, so we have (1/3 x 1/2) + (1/3 x 1/3), or 1/3d + 1/3d where d = 2 in the first roll, and d = 3 in the second because the size of d depend on the first roll. Why is the second roll in the first element 1/2? Because we had a 1/3 probability of rolling a 2 first roll, and since we rolled a 2 that determined the size of the second roll which was a 2 sided die. In the other event we had a 1/3 probability of rolling a 3, obtained a 3 meaning the second roll is a 3 sided die, so have another 1/3 probability of rolling a 2 on the second roll. 
</p>

<p>
15.9 Looking at the solution for if A is a subset of B, then Pr[A] is less or equal to Pr[B], they first translate the notation Pr[A] to it's definition which is the sum of all outcomes (ie: the leaves, or the probabilities) in set A. Add to this the set B minus A, and you get the set B because (B\A) + A = B, so Pr[A] must be less than or equal to Pr[B]. Make examples using rolls of single 6-sided dice:
</p>
<ul class="org-ul">
<li>Sample space = {1,2,3,4,5,6} and Pr(l) of any element in set Omega = 1/6</li>
<li>B is an event subset of omega: B = {1,2}</li>
<li>A is a subset of B: A = {1}
<ul class="org-ul">
<li>Pr[A] + Pr[B\A] is 1/6 + 1/6</li>
<li>Pr[B] is 1/6 + 1/6</li>
</ul></li>
</ul>
<p>
The rest of 15.9 is pretty obvious, make toy examples using the same sets from our dice rolling example:
</p>
<ul class="org-ul">
<li>A = {1}, Pr[A] = 1/6</li>
<li>B = {1,2}, Pr[B] = 1/6 + 1/6
<ul class="org-ul">
<li>A union B is {1,2} or 1/6 + 1/6.</li>
<li>A intersect B = {1}, or 1/6.</li>
<li>Pr[A] + Pr[B] is 1/6 x 3, so minus their intersection and we get the desired 1/6 + 1/6.</li>
</ul></li>
</ul>

<p>
15.11 make another example for yourself using the 15.8 exercise 'I will roll double numbers'. If you broke up that set into individual sets of A1 = {(1,1)}, A2 = {(2,2)} then their union is {1,2} but Pr[A1] + Pr[A2] is bigger than the union as Pr[A1] + Pr[A2] is 4/36 where Pr[A1 union A2] is 2/36.
</p>

<p>
15.14 this is 1/3 x d again or 1/3d probability. As you can see from the solution, it's just plugging in event sets to the formula for Pr[A|B].
TODO
</p>
</details>

<hr>
<p>
<a href="./index.html">Home</a>
</p>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
