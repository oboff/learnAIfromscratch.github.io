<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2021-05-06 Thu 23:39 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Artificial Intelligence</title>
<meta name="generator" content="Org mode">
<meta name="author" content="jbh">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  body {
  max-width: 40rem;
  padding: 1rem;
  margin: auto;
  }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<style> body {background-color: #fafad2} </style>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<header>
<h1 class="title">Artificial Intelligence</h1>
</header><nav id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#Courses">Courses</a>
<ul>
<li><a href="#Books">Books</a></li>
<li><a href="#Papers">Papers</a></li>
</ul>
</li>
<li><a href="#Lectures%201">Lectures 1</a>
<ul>
<li><a href="#Probability">Probability</a></li>
<li><a href="#Reading">Reading</a></li>
<li><a href="#Parameters">Parameters</a></li>
<li><a href="#PML%20book%20chapter%201">PML book chapter 1</a></li>
</ul>
</li>
<li><a href="#Lectures%202">Lectures 2</a>
<ul>
<li><a href="#10-601%20Decision%20Trees%20Part%201">10-601 Decision Trees Part 1</a></li>
</ul>
</li>
</ul>
</div>
</nav>

<div id="outline-container-Courses" class="outline-2">
<h2 id="Courses">Courses</h2>
<div class="outline-text-2" id="text-Courses">
<p>
Let's start with machine learning. Sometimes watching the lectures is necessary since the prof will sketch out an algorithm that isn't in the slides but like all lectures they aren't as long as the running time, a lot of in-class student work or review you can skip.   
</p>

<ul class="org-ul">
<li>10-701 (Fa 2020) <a href="https://www.cs.cmu.edu/~epxing/Class/10701-20/schedule.html">Intro to Machine Learning</a> has YouTube lectures/assignments open to us</li>
<li>10-301/601 (Sp 2020) <a href="http://www.cs.cmu.edu/~mgormley/courses/10601/schedule.html">Intro to Machine Learning</a> has YouTube <a href="https://www.youtube.com/playlist?list=PLpqQKYIU-snAPM89YPPwyQ9xdaiAdoouk">lectures</a> and open assignments</li>
<li>CS4780 (2018) <a href="https://www.youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS">ML for Intelligent Systems</a> YouTube lectures w/<a href="http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/">notes</a></li>
<li>Wildberger's <a href="https://www.youtube.com/playlist?list=PLIljB45xT85AMigTyprOuf__daeklnLse">playlist</a> of probability &amp; stats a geometric explanation</li>
</ul>

<p>
The courses use C++ Python Java Octave but I'm using Julia simply because there exists excellent resources all written by MIT profs
</p>

<ul class="org-ul">
<li>MIT's <i>Introduction to Computational Thinking</i> <a href="https://computationalthinking.mit.edu/Spring21/">18.S191</a> 2021 course
<ul class="org-ul">
<li>Amazing set of lectures that explain things like the SVD (linear algebra) and probability
<ul class="org-ul">
<li>ML w/Julia <a href="https://juliaacademy.com/p/introduction-to-machine-learning">tutorial</a> for using the libraries</li>
<li>Some of a <a href="https://mitmath.github.io/18337/">course</a> w/YouTube lectures on AI performance engineering</li>
</ul></li>
</ul></li>
</ul>


<p>
Some backups  
</p>
<pre class="example" id="org8d1b97f">
Torrent for 10-701 lectures/assignments
magnet:?xt=urn:btih:40ea1c0bb1dbbef33e2f7ffb5df0106f03d50a3e&amp;dn=10701

Torrent for 10-301/601 lectures/assignments
magnet:?xt=urn:btih:2e1005d058b5f4c357d7338c85937aabdd91dcdc&amp;dn=10601
</pre>
</div>

<div id="outline-container-Books" class="outline-3">
<h3 id="Books">Books</h3>
<div class="outline-text-3" id="text-Books">
<p>
The courses do not follow a single book and recommend the following, many are free or try <a href="https://en.wikipedia.org/wiki/Library_Genesis">libgen</a>:
</p>

<ul class="org-ul">
<li><i>Machine Learning A Probabilistic Perspective</i> by Kevin Murphy
<ul class="org-ul">
<li>2021 <a href="https://probml.github.io/pml-book/book1.html">draft</a> is free &amp; includes Jupyter <a href="https://github.com/probml/pyprobml/tree/master/book1">notebooks</a> for each chapter</li>
</ul></li>
<li><i>Pattern Recognition and Machine Learning</i> by Bishop is free <a href="https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/">here</a></li>
<li><i>Machine Learning</i> by Tom Mitchell w/ <a href="http://www.cs.cmu.edu/%7Etom/NewChapters.html">new</a> chapters</li>
<li><i>The Elements of Statistical Learning</i> is <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">free</a></li>
</ul>

<p>
General modeling books   
</p>
<ul class="org-ul">
<li><i>Mathematical Modeling and Applied Calculus</i> reviewed <a href="https://www.maa.org/press/maa-reviews/mathematical-modeling-and-applied-calculus">here</a></li>
<li><i>Principles of Applied Statistics</i> by Cox/Donnelly reviewed <a href="http://bactra.org/reviews/cox-donnelly.html">here</a></li>
</ul>

<p>
Mathematics reference
</p>
<ul class="org-ul">
<li><i>Mathematics for Machine Learning</i> by Deisenroth/Faisal/Ong is <a href="https://mml-book.github.io/">free</a></li>
</ul>
</div>
</div>

<div id="outline-container-Papers" class="outline-3">
<h3 id="Papers">Papers</h3>
<div class="outline-text-3" id="text-Papers">
<ul class="org-ul">
<li>Depth First Learning builds <a href="https://www.depthfirstlearning.com/">curriculums</a> around papers to understand them</li>
<li>MIT's graduate section for 6.034 on interesting AI <a href="https://ai6034.mit.edu/wiki/index.php?title=6.S966:_A_Graduate_Section_for_6.034">papers</a></li>
</ul>

<blockquote>
<p>
"Recent astonishing progress in "machine learning" has eclipsed much of the traditional work on symbolic thinking. But problems remain: the systems that result from work on machine learning research have no concept of meaning&#x2013;the "words" do not have referents outside of the ways in which they are used. Such systems may perform well on many tasks but they do not smoothly interface with systems that are organized around modeling the world, which is probably essential to solving really deep problems of common sense and science." - <a href="https://ai6034.mit.edu/wiki/index.php?title=6.S966:_A_Graduate_Section_for_6.034">Gerald Sussman</a>
</p>
</blockquote>
</div>
</div>
</div>


<div id="outline-container-Lectures%201" class="outline-2">
<h2 id="Lectures%201">Lectures 1</h2>
<div class="outline-text-2" id="text-Lectures%201">
<p>
Let's audit the first intro lectures for CS4780, 10-701, and 10-601.
</p>


<p>
10-601 primarily talked about history of ML like expert systems and why they failed. We learn the brief difference between regression output, probability output and classification. Most of this lecture is course logistics you can skip.
</p>

<p>
10-701 when you get past course logistics talks about how machine learning constantly changes, there was a breakthrough in neural networks recently so now it's all about neural networks, we used to heavily use kernel methods and now do not. We could use convex optimization in the past and now things are much more non-convex.  
</p>

<p>
CS4780 the <a href="https://youtu.be/MrLPzBxG95I">intro</a> lecture jumps into supervised learning setup, explaining the math notation. His high level explanation of how machine learning works: insert data, insert expected output, generate program. At 32:00 the supervised learning intro begins. Lecture notes are <a href="http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/">here</a>. He uses arrows on top of the notation to denote vectors. Look up what <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">iid</a> is: "all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples". \(D = \{ (x_{i}, y_{i})....(x_{n},y_{n}) \}\) is a subset of XY meaning a cartesian product of every single combination of X (features) with Y (labels), like the cartesian coordinates in a x and y graph, you can combine any one of them to form a point: (0,1) or (100,-1), so using his patient data vector example: Y would be binary 0 or 1 to represent yes or no, X is the feature set of a patient and D is the total training data set of all possible combinations of feature set with yes and no.
</p>

<p>
You can learn basic set theory by typing it into <a href="https://www.youtube.com/watch?v=59WX2V7Vjgg">YouTube</a> and vectors are covered in the reading, or the book <i>Mathematical Modeling and Applied Calculus</i> from the beginning of these notes.
</p>
</div>

<div id="outline-container-Probability" class="outline-3">
<h3 id="Probability">Probability</h3>
<div class="outline-text-3" id="text-Probability">
<p>
Wildberger's probability lecture <a href="https://youtu.be/siXj4hkUIp8">ProbStats5: Random variables</a> will explain notation like \(X^{-1}(x_i)\). A random variable is a function over the sample space that returns a real number and he geometrically demonstrates this function. Expected value E(x) is demonstrated as the mean, variance Var(x) is demonstrated as the distance between probabilities and the mean, and standard derivation SD(x) turns the variance from quadratic to linear. Probability is covered extensively by all the books the courses recommend but this is a nice lecture to visualize a random variable and all the other Wildberger lectures are similar first presenting a visualization of the theory then examples.
</p>
</div>
</div>

<div id="outline-container-Reading" class="outline-3">
<h3 id="Reading">Reading</h3>
<div class="outline-text-3" id="text-Reading">
<p>
Let's skim chapter 8.1 <i>When Models Meet Data</i> of the <a href="https://mml-book.github.io/">mml book</a>. We take tabular data, represent it as a vector with N rows (samples) and D columns (features). Once we have a vector representation we can use concepts from linear alegbra to manipulate it, and to compare it to another vector by constructing a geometry we can run optimizations on. We read about different ways to construct new features and that increasingly neural networks are used to learn new features which was mentioned in the first lecture of 10-701. There are two schools: machine learning where the predictor is a function or a probability model, we will mainly be covering the probability model in 10-601/701. In 8.1.4 the goal is to construct a model so that it's parameters will perform well on new unseen data.
</p>
</div>
</div>

<div id="outline-container-Parameters" class="outline-3">
<h3 id="Parameters">Parameters</h3>
<div class="outline-text-3" id="text-Parameters">
<p>
What is a parameter? In the chapter on vector calculus, these are described as 'an optimization problem: find parameters that control how well a model explains the data' and there is a concrete example of a curve model of training data, where at each observation we want to find parameters that explain the observations basically finding a signal in noise. Skimming through chapter 7 of <i>Principles of Applied Statistics</i> on criteria for parameters: "we aim to summarize the aspects of interest by parameters, preferably small in number and formally defined as properties of the probability model". There's some examples here like a pipe with water flowing through it, if you take measurements what is the minimum amount of measurements to include as parameters to your water pipe model that best describes the observations like velocity, the area inside the pipe, the viscosity of the water (feature set). The choice of regression model is explained as the search for a model with as few explanatory variables as necessary to give an adequate empirical fit to the data where overfitting means your model too closely resembles the sample and can't be generalized. Summary: parameters are what you specify some object with, like if you were to specify a cat object, what are the minimum amount of cat characteristics you could use to distinguish the object as a cat from other objects. 
</p>
</div>
</div>

<div id="outline-container-PML%20book%20chapter%201" class="outline-3">
<h3 id="PML%20book%20chapter%201">PML book chapter 1</h3>
<div class="outline-text-3" id="text-PML%20book%20chapter%201">
<p>
Let's also skim the first chapter in the 2021 <a href="https://probml.github.io/pml-book/book1.html">book</a> <i>Probabilistic Machine Learning</i>. Each chapter also has a colab notebook you can run <a href="https://github.com/probml/pyprobml/tree/master/book1">here</a> in Python. This is a terse survey of what we are about to take in detail and much of this is what we just saw in CS4780's first lecture. All of these stats functions exist as libraries in programming language of choice.
</p>

<p>
Reading 1.2 <i>Supervised Learning</i>, if you took the compsci <a href="https://learnaifromscratch.github.io/software.html">workshop</a> you know exactly what tabular data and fixed-size feature representation is as it was the first assignment we did turning documents into vectors and <a href="https://www.khanacademy.org/computing/computer-programming/programming-natural-simulations/programming-vectors/a/vector-magnitude-normalization">normalizing</a>. The 'design matrix' example in Murphy's book notes that this is what <i>big data</i> means because N (150 rows) is greater than D (4 columns), the number of features. Figure 1.4 we have simple if-else statements represented as a decision tree, value = [0, 49, 5] for versicolor means our decision tree only correctly labelled 49 versicolors and mislabelled 5 virginica. 
</p>

<p>
The decision rule function f(x;theta), I read it as x is the input, consuming that entire table of data for the flowers as a vector, and theta (the parameters) are features and their threshold to define the decision function like feature 'petal-length' &lt;= 2.45.
</p>

<p>
1.2.1.4 the goal of supervised learning is coming up with a classification model like the decision tree, to reliably predict labels for any new data. The loss function notation is explained in the appendix. Reading from left to right, Loss-Fun(params) is defined as the average of the sum of all outputs from a binary indicator function so summing 0 and 1's then dividing by the total number of indices of input vector x to get the average. The binary indicator function is a boolean comparing the target y (the known label) to the predicted label output of f(x;params) as you iterate the row indices of that table of data. This is explained in <i>Mathematics for Machine Learning</i> in chapter 8.3.1, or search the entire pdf for negative log-likelihood. According to that book the interpretation should be if you vary the parameters on fixed data (our vector input of flower features) this function tells us how likely a particular setting of the parameters is to explain the observations in the input vector. Example 8.5 of that book shows this as a least-squares problem and both log(), exp() and least squares are explained in the <i>Mathematical Modeling and Applied Calculus</i> text. 
</p>


<p>
In the following function the hat notation on top of the y means estimated/predicted value of y. That entire function is then further reduced to the notation \(l_{01}(y,\hat{y})\) to indicate zero-one loss of true label vs predicted label. The last function empirical risk minimization, argmin is defined <a href="https://math.stackexchange.com/a/2157524">here</a> meaning what x input to f(x) results in the global minimum (lowest y value) so imagine that you are feeding a decision rule function with numerous different thresholds/features as parameters, that argmin function should return the parameters with the least average mislabeling on the training set.
</p>

<p>
Some conditional probability introduced here that are in the above Wildberger lectures. Equation 1.7 if you look at the appendix means the 'parametric distributions' or probability that y = label given random variable x and parameters, and it's equated to more terse notation of \(f_{c}(x;\theta)\). That p() function returns a number from 0 to 1, in this example 3 ordered elements of a list that represent the 3 labels, with the probability of them being labelled setosa 0/54 so highly unlikley to happen, and the probability of being labelled versicolor 49/54 or 0.9 so very high probability but there is still uncertainty. This probability function output is then input to NLL() and if you were to plot a negative log() function in R/Python/Julia, you would see that when given a higher number like 1, the graph is very close to the x-axis (minimum loss) whereas if probability is closer to 0, the graph shoots up to infinity on the y-axis meaning a lot of loss. MLE is explained in the <i>Mathematics for ML</i> book in chapter 8.3.1 "MLE gives us the most likely parameter for the set of data" and there's lectures regarding MLE in 10-601/701.
</p>
</div>
</div>
</div>

<div id="outline-container-Lectures%202" class="outline-2">
<h2 id="Lectures%202">Lectures 2</h2>
<div class="outline-text-2" id="text-Lectures%202">
</div>
<div id="outline-container-10-601%20Decision%20Trees%20Part%201" class="outline-3">
<h3 id="10-601%20Decision%20Trees%20Part%201">10-601 Decision Trees Part 1</h3>
<div class="outline-text-3" id="text-10-601%20Decision%20Trees%20Part%201">
<p>
Watching 10-601 second lecture <i>Decision Trees (Part 1)</i>. There's secret <a href="https://1drv.ms/o/s!Aqk9RupCw3gqhnEVySsGVwiAwMI6">notes</a> and also a math notation <a href="http://www.cs.cmu.edu/~mgormley/courses/10601-s20/slides/10601-notation.pdf">crib sheet</a> that is extremely helpful. 
</p>

<p>
So far, if you took CS19 from the compsci workshop then all of this is familiar ie: table of data, function type notation. If it seems unclear, the c*(x) function is whatever generated the training data, so a doctor providing their medical diagnosis history over x months with hundreds of patients and c*(x) is the function in the doctor's brain that generated the decisions in the training data, and h(x) is what we want to build that best approximates c*(x).
</p>

<p>
@37:00 or so we get introduced to loss functions we already read about in the first chapter in the 2021 <a href="https://probml.github.io/pml-book/book1.html">book</a> <i>Probabilistic Machine Learning</i> except he's using the compact abstract notation. We have to write a decision stump algorithm in the homework, write down it's spec he gives in lecture since it's not in the assignment writeup for some reason. The website schedule for the course has a link to <a href="http://ciml.info/dl/v0_99/ciml-v0_99-ch01.pdf">this</a> reading which is exactly what we already read though they explain the notation better.
</p>

<p>
<b>Assignment 1</b>
</p>

<p>
Try the <a href="https://gitlab.com/mathfromtheverybeginning/homework-10301/-/tree/master/hw1">homework</a>, it has an autograder for some languages but you don't need it. All these functions like decision stumps already exist in highly optimized <a href="https://github.com/bensadeghi/DecisionTree.jl/blob/master/README.md">libraries</a> so I implemented the assignment using a few filters in <a href="https://code.pyret.org/">Pyret</a>. You can import the testing and training data as google sheets, convert them to tables then extract and filter columns.  Anything in the homework files ending in .labels is the solution which you can literally cut and paste into google sheets, import as a table in Pyret and then run a test case to see if you pass though you won't need to this is a simple assignment. 
</p>

<pre class="example" id="org47624fb">
include gdrive-sheets

ssid = "1w53u8p8eiyQ5efww54yz9b3WHcFgPMQe-Em0oLG5AJ0"
politicians-train =
  load-table: Anti_satellite_test_ban, Aid_to_nicaraguan_contras, Mx_missile, Immigration, Superfund_right_to_sue, Duty_free_exports,	Export_south_africa, Party
    source: load-spreadsheet(ssid).sheet-by-name("politicians_train", true)
  end

ssid2 = "1baH3PUQ852FSHZJ063v-ozi0wFARXEHIKIPgHCyqVVY"
politicians-test =
  load-table: Anti_satellite_test_ban, Aid_to_nicaraguan_contras, Mx_missile, Immigration, Superfund_right_to_sue, Duty_free_exports,	Export_south_africa, Party
    source: load-spreadsheet(ssid2).sheet-by-name("politicians_test", true)
  end
</pre>

<p>
TODO
</p>



<hr>
<p>
<a href="./index.html">Home</a>
</p>
</div>
</div>
</div>
</div>
</body>
</html>
